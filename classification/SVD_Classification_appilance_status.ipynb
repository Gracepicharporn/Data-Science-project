{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gracepicharporn/Data-Science-project/blob/main/classification/SVD_Classification_appilance_status.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differnt method and applied SVD"
      ],
      "metadata": {
        "id": "IHYKbYmozqtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UK-DALE"
      ],
      "metadata": {
        "id": "g9VyDfRvzwKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building1"
      ],
      "metadata": {
        "id": "-zi4_ZyOz0j5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyIDjHbY3JUE"
      },
      "source": [
        "Dowload Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ySv17MU1ik2",
        "outputId": "b67d218e-c2d4-402c-cfbb-8abeb055cfc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MpNhkV-76eYj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CNcc8lAgB5Qm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image with fridge**"
      ],
      "metadata": {
        "id": "4_wUPEWdn2va"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsifLo83BDax",
        "outputId": "82a0ca7a-65ce-4de7-8047-4ce191641e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:356: UserWarning: This ImageDataGenerator specifies `samplewise_std_normalization`, which overrides setting of `samplewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2608 images belonging to 2 classes.\n",
            "Found 650 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "dir = '/content/gdrive/MyDrive/data-device/SVD-fridge'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      validation_split = 0.2)\n",
        "\n",
        "train_generator =train_datagen.flow_from_directory(\n",
        "     dir,  # train folder\n",
        "     batch_size = 64,  # define batch size\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     subset = 'training',\n",
        "     target_size = (224,224)\n",
        "\n",
        ")\n",
        "\n",
        "test_generator =train_datagen.flow_from_directory(\n",
        "     dir,  # test folder\n",
        "     batch_size= 1,  # define batch size (1 for test dataset)\n",
        "     shuffle=True,  \n",
        "     subset = 'validation',\n",
        "     target_size = (224, 224)\n",
        "\n",
        ")\n",
        "                    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqsK1XDDEJKp",
        "outputId": "44d3f03e-00c5-47ec-c16a-fb5ea5287302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Shape \n",
            " (64, 224, 224, 3)\n",
            "Test Shape \n",
            " (1, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "# shape of dataset\n",
        "trainShape=train_generator.__getitem__(0)[0].shape  # train shape\n",
        "testShape=test_generator.__getitem__(0)[0].shape  # test shape\n",
        "print(\"Train Shape \\n\",trainShape)\n",
        "print(\"Test Shape \\n\",testShape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DXo-IFPRFtfa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16  # VGG16 pre-trained model\n",
        "import tensorflow as tf  # for deep learning\n",
        "from tensorflow.keras import Model  # for creating a model\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # for calculating precision, recall, f1-score, and accuracy\n",
        "import pandas as pd  # for creating a table\n",
        "import numpy as np  # for calculating numeric values\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping  # for learning rate reduction and early stopping\n",
        "lr_reduce = ReduceLROnPlateau(monitor='accuracy', patience=1)  # learning rate reduction\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)  # early stopping\n",
        "\n",
        "def pretrained_model(model):\n",
        "  '''\n",
        "    Create a model from a pre-trained model by adding 1 3-nodes dense layer on top\n",
        "    of the model\n",
        "    Input:\n",
        "      model: a pre-trained model\n",
        "    Output:\n",
        "      a model \n",
        "  '''\n",
        "  # define a pre-trained model without the top layers, and with the default weights\n",
        "  pre_trained_model = model(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')\n",
        "  pre_trained_model.trainable = False  # freeze the weights\n",
        "  pre_trained_model.summary()\n",
        "\n",
        "  x = tf.keras.layers.Flatten()(pre_trained_model.output)  # flatten layer\n",
        "  x = tf.keras.layers.Dense(2 , activation='sigmoid')(x)  # 1 3-nodes dense layer\n",
        "  model = Model(pre_trained_model.input, x) \n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def train(model):\n",
        "  '''\n",
        "    train the model\n",
        "    Input:\n",
        "      model: a model\n",
        "    Output:\n",
        "      history of the training process\n",
        "  '''\n",
        "  # compile the model with specific configuration\n",
        "  model.compile(optimizer='adam', loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
        "  # train the model for 10 epochs and 30 steps for each epoch\n",
        "  history = model.fit(train_generator, validation_data = test_generator, epochs = 30, \n",
        "                      callbacks=[lr_reduce, early_stop])\n",
        "  return history\n",
        "\n",
        "def plot_train_evaluation(history, model):\n",
        "  '''\n",
        "    Plot train loss and acuracy. Then, evaluate the model (precision, recall, f1-score, and accuracy) with test dataset\n",
        "    Input:\n",
        "      history: a history of the training process\n",
        "      model: a model\n",
        "    Output:\n",
        "      test report (DataFrame)\n",
        "  '''\n",
        "  # train loss and accuracy plot\n",
        "  plt.figure(figsize=(12, 8))  # define figure size\n",
        "  plt.subplot(2, 2, 1)  # create a subplot\n",
        "  plt.plot(history.history['val_accuracy'], label='val_accuracy') \n",
        "  plt.plot(history.history['accuracy'], label='train_accuracy') \n",
        "  plt.legend()  # plot legend\n",
        "  plt.xlabel('epochs')  # define x label\n",
        "  plt.title('Accuracy plot')  # define title\n",
        "\n",
        "  plt.subplot(2, 2, 2)  # create a subplot\n",
        "  plt.plot(history.history['val_loss'], label='val_loss') \n",
        "  plt.plot(history.history['loss'], label='train_loss') \n",
        "  plt.legend()  # plot legend\n",
        "  plt.xlabel('epochs')  # define x label\n",
        "  plt.title('Loss plot')  # define title\n",
        "  plt.show()  # show all plots\n",
        "\n",
        "  # print out average, max, and min of train accuracy and loss\n",
        "  print('average train accuracy: ', sum(history.history['accuracy'])/len(history.history['accuracy']))\n",
        "  print('max train accuracy: ', max(history.history['accuracy']))\n",
        "  print('average train loss: ', sum(history.history['loss'])/len(history.history['loss']))\n",
        "  print('min train loss: ', min(history.history['loss']))\n",
        "\n",
        "  print('#########################################################')\n",
        "  # print out average, max, and min of validation accuracy and loss\n",
        "  print('average train accuracy: ', sum(history.history['val_accuracy'])/len(history.history['val_accuracy']))\n",
        "  print('max train accuracy: ', max(history.history['val_accuracy']))\n",
        "  print('average train loss: ', sum(history.history['val_loss'])/len(history.history['val_loss']))\n",
        "  print('min train loss: ', min(history.history['val_loss']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y09tRnYGV96",
        "outputId": "686899a6-7fac-4d85-dd65-62d63251da12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 50178     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,764,866\n",
            "Trainable params: 50,178\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# VGG16 pre-trained model\n",
        "model1 = pretrained_model(VGG16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQCD35eEGatj",
        "outputId": "9a98f100-61f3-45fa-d434-b42a225f1c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            " 8/41 [====>.........................] - ETA: 5:02 - loss: 0.7999 - accuracy: 0.5273"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "history1 = train(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ngo2FWGiqW"
      },
      "outputs": [],
      "source": [
        "# training and testing results\n",
        "plot_train_evaluation(history1, model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzKimJGWKvbZ"
      },
      "outputs": [],
      "source": [
        "model1.save(\"svd_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYzlDJdQeHPR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications import VGG16  \n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import Model  \n",
        "from sklearn.metrics import confusion_matrix, classification_report  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n7TmmcJek_k"
      },
      "outputs": [],
      "source": [
        "model2 = load_model('svd_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRHUzBKpe8xW"
      },
      "outputs": [],
      "source": [
        "model2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kettle**"
      ],
      "metadata": {
        "id": "GdWa8dxJoBQl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIFkVQeOe9nY"
      },
      "outputs": [],
      "source": [
        "dir2 = '/content/gdrive/MyDrive/data-device/SVD-kettle'\n",
        "train_datagen2 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator2 = train_datagen2.flow_from_directory(\n",
        "     dir2,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict1 = model2.evaluate(train_generator2, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "g1ikKcO0HM3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict1 "
      ],
      "metadata": {
        "id": "fMGZotbdPDia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = load_model('svd_model.h5')"
      ],
      "metadata": {
        "id": "hleebbRnIrAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Washer Dryer**"
      ],
      "metadata": {
        "id": "nxqQ9k2noD6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir3 = '/content/gdrive/MyDrive/data-device/SVD-washer'\n",
        "train_datagen3 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator3 = train_datagen3.flow_from_directory(\n",
        "     dir3,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "hkafLOnBOZAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict2 = model3.evaluate(train_generator3, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "Fqx9HVFCOqI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict2"
      ],
      "metadata": {
        "id": "n4qBUsTQPdH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Across building"
      ],
      "metadata": {
        "id": "U1GKLjTo0AKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fridge**"
      ],
      "metadata": {
        "id": "BgxXcjJaz-71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir4 = '/content/gdrive/MyDrive/data-device/UK-DALE-data/diff/SVD-across-house/train-fridge-SVD'\n",
        "train_datagen4 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator4 = train_datagen4.flow_from_directory(\n",
        "     dir4,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "8yCi6hMXz_yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict4 = model3.evaluate(train_generator4, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "MgTYzwb40GzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict4"
      ],
      "metadata": {
        "id": "AZg7Lvzx0H6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kettle**"
      ],
      "metadata": {
        "id": "Wjyq8Lr-1EIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir5 = '/content/gdrive/MyDrive/data-device/UK-DALE-data/diff/SVD-across-house/train-kettle-SVD'\n",
        "train_datagen5 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator5 = train_datagen5.flow_from_directory(\n",
        "     dir5,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "Nwmdf0ue1FyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict5 = model3.evaluate(train_generator5, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "fR1RPquR1Jf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict5"
      ],
      "metadata": {
        "id": "sWGI0dKw1NL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Washer Dryer**"
      ],
      "metadata": {
        "id": "VRqnPlnx1ZJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir6 = '/content/gdrive/MyDrive/data-device/UK-DALE-data/diff/SVD-across-house/train-washer-SVD'\n",
        "train_datagen6 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator6 = train_datagen6.flow_from_directory(\n",
        "     dir6,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "g2GeFpdM1cHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict6 = model3.evaluate(train_generator6, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "stDIeXG613kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict6"
      ],
      "metadata": {
        "id": "ItgYuXR714I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REDD"
      ],
      "metadata": {
        "id": "mMJn2RwK18NL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building 1"
      ],
      "metadata": {
        "id": "E-JDCaus1-Rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fridge**"
      ],
      "metadata": {
        "id": "LbqK-0up2Bai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir7 = '/content/gdrive/MyDrive/data-device/REDD-data/diff/buildind1-SVD/train-fridge-svd'\n",
        "train_datagen7 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator7 = train_datagen7.flow_from_directory(\n",
        "     dir7,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "Mo9NS2_u15pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict7 = model3.evaluate(train_generator7, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "PTPA3jS22T72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict7 "
      ],
      "metadata": {
        "id": "5uBDaJaU2VzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Microwave**"
      ],
      "metadata": {
        "id": "yNzzTHAW2Wcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir8 = '/content/gdrive/MyDrive/data-device/REDD-data/diff/buildind1-SVD/train-microwave-svd'\n",
        "train_datagen8 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator8 = train_datagen8.flow_from_directory(\n",
        "     dir8,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "1Lg7b_Yb2YWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict8 = model3.evaluate(train_generator8, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "JVWsNx9H2gfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict8 "
      ],
      "metadata": {
        "id": "CfTeFajw2hJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Washer Dryer**"
      ],
      "metadata": {
        "id": "83Ki63GM2l6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir9 = '/content/gdrive/MyDrive/data-device/REDD-data/diff/buildind1-SVD/train-washer-svd'\n",
        "train_datagen9 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator9 = train_datagen9.flow_from_directory(\n",
        "     dir9,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "ToZa3b512mVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict9 = model3.evaluate(train_generator9, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "vAg_cEja2wqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict9 "
      ],
      "metadata": {
        "id": "SCugj_au21h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Across builindg"
      ],
      "metadata": {
        "id": "BS_iGhR127LI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fridge**"
      ],
      "metadata": {
        "id": "qDEXAAHI2_YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir10 = '/content/gdrive/MyDrive/data-device/REDD-data/diff/across-builg-SVD/SVD-train_fridge'\n",
        "train_datagen10 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator10 = train_datagen10.flow_from_directory(\n",
        "     dir10,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "0JppwSDh25hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict10 = model3.evaluate(train_generator10, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "qxdAnZql25_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict10"
      ],
      "metadata": {
        "id": "R8ci_Ynt3Jiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Microwave**"
      ],
      "metadata": {
        "id": "VwthIVlS3Mek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir11 = '/content/gdrive/MyDrive/data-device/REDD-data/diff/across-builg-SVD/SVD-train_microwave'\n",
        "train_datagen11 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator11 = train_datagen11.flow_from_directory(\n",
        "     dir11,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "sO_tn3vb3OoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict11 = model3.evaluate(train_generator11, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "9xXP18kz3Zwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict11"
      ],
      "metadata": {
        "id": "Jk9QhHRk3aeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Washer Dryer**"
      ],
      "metadata": {
        "id": "e42LKgZo3cef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir12 = '/content/gdrive/MyDrive/data-device/REDD-data/diff/across-builg-SVD/SVD-train_washer'\n",
        "train_datagen12 = ImageDataGenerator(\n",
        "      samplewise_std_normalization= True,\n",
        "      )\n",
        "\n",
        "train_generator12 = train_datagen12.flow_from_directory(\n",
        "     dir12,  # train folder\n",
        "     shuffle=True,  # shuffle the train dataset\n",
        "     target_size = (224,224)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "l_lookxC3d82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict12 = model3.evaluate(train_generator12, return_dict = True, use_multiprocessing= True)"
      ],
      "metadata": {
        "id": "mYlL_zO239S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict12"
      ],
      "metadata": {
        "id": "OJejDCj93-ye"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SVD_Classification_appilance_status.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}